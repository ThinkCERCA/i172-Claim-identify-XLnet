{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZP0MCuco9Dj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install environment"
      ],
      "metadata": {
        "id": "g5rcL7i3pGtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "8JvAIoTepJwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0158113-d778-40ac-eccc-9c289f252777"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 7.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 7.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload data to colab"
      ],
      "metadata": {
        "id": "oVqkdlhopbeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after shift+enter, you have to click choose file,choose the 'tr.csv'\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "_yzSmlihpaxn",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "5030cd65-9aa9-42f5-bb09-ed80df3bb5ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4d448a9b-f8a0-4065-998b-dfcfa648c565\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4d448a9b-f8a0-4065-998b-dfcfa648c565\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tr.csv to tr.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use updated data"
      ],
      "metadata": {
        "id": "1GQMpdI9J-QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "with open('i172_7000_vs3_label.txt','r') as f:\n",
        "  claim = f.readlines()\n",
        "labels = [np.int(t.strip()) for t in claim]\n",
        "\n",
        "with open('i172_7000_vs1_text.txt','r') as f:\n",
        "  texts = f.readlines()\n",
        "texts = [t.strip() for t in texts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scj_P6zOhmlr",
        "outputId": "38f3ec04-6a3f-4e34-ede2-630dfcae3a13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8EcyfD7iJWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read data"
      ],
      "metadata": {
        "id": "-BpcRaBhplLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"tr.csv\")"
      ],
      "metadata": {
        "id": "dSmMBqoLpnKV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick 7000 data as training data\n",
        "texts = df.irsen_text.values.tolist()[:7000]\n",
        "labels = df.claim_s.values.tolist()[:7000]\n",
        "# 1 of the evidence has wrong label, delete. Now we have 6999 training data\n",
        "# labels = labels[:4487] + labels[4488:7000]\n",
        "# texts = texts[:4487] + texts[4488:7000]\n",
        "len(texts)"
      ],
      "metadata": {
        "id": "4R_8GsvEpuAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52528f31-72b6-44d4-c64c-45bc724119c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GPU"
      ],
      "metadata": {
        "id": "7VGUdGbwp_-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "azBTb1v0qMBo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data prepare"
      ],
      "metadata": {
        "id": "eSFgKj7aujXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data to training and validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,recall_score,precision_score,accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, random_state=2, test_size=0.1)"
      ],
      "metadata": {
        "id": "NMl-x7PbstIi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer, turn data to bert type token\n",
        "from transformers import BertTokenizerFast,RobertaTokenizer\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "QwGoUMnxugJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369b92ec-c778-46fd-c15c-72aaae700e05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn data to torch dataset\n",
        "class bertDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = bertDataset(train_encodings, train_labels)\n",
        "val_dataset = bertDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "MS_dGFb7vCvv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare model"
      ],
      "metadata": {
        "id": "FrpFOiHEvP4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training metrics, will show result during training\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "id": "8NNL4mr3vPLN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from torch import nn\n",
        "\n",
        "# set some parameter\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    learning_rate=2e-05,\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "# using XLnet classification model\n",
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\")\n",
        "# using bert classification model\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device) # put model to gpu\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,# evaluation dataset\n",
        "    compute_metrics=compute_metrics,             \n",
        "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "4eh0LhPIxdQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf1b32e6-bb5e-4955-9b89-cd6a84094a24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 6300\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1576\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1576' max='1576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1576/1576 37:32, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.434000</td>\n",
              "      <td>0.467440</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.345000</td>\n",
              "      <td>0.193675</td>\n",
              "      <td>0.932857</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.728324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.222900</td>\n",
              "      <td>0.216816</td>\n",
              "      <td>0.957143</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.775510</td>\n",
              "      <td>0.835165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>0.377784</td>\n",
              "      <td>0.922857</td>\n",
              "      <td>0.650685</td>\n",
              "      <td>0.969388</td>\n",
              "      <td>0.778689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.130461</td>\n",
              "      <td>0.972857</td>\n",
              "      <td>0.876190</td>\n",
              "      <td>0.938776</td>\n",
              "      <td>0.906404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.230700</td>\n",
              "      <td>0.116769</td>\n",
              "      <td>0.967143</td>\n",
              "      <td>0.844037</td>\n",
              "      <td>0.938776</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.158700</td>\n",
              "      <td>0.111676</td>\n",
              "      <td>0.977143</td>\n",
              "      <td>0.936170</td>\n",
              "      <td>0.897959</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.165300</td>\n",
              "      <td>0.084974</td>\n",
              "      <td>0.977143</td>\n",
              "      <td>0.945652</td>\n",
              "      <td>0.887755</td>\n",
              "      <td>0.915789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.154400</td>\n",
              "      <td>0.109907</td>\n",
              "      <td>0.965714</td>\n",
              "      <td>0.813559</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.065600</td>\n",
              "      <td>0.118698</td>\n",
              "      <td>0.974286</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.867347</td>\n",
              "      <td>0.904255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.167200</td>\n",
              "      <td>0.051604</td>\n",
              "      <td>0.982857</td>\n",
              "      <td>0.967391</td>\n",
              "      <td>0.908163</td>\n",
              "      <td>0.936842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.061700</td>\n",
              "      <td>0.077323</td>\n",
              "      <td>0.981429</td>\n",
              "      <td>0.920792</td>\n",
              "      <td>0.948980</td>\n",
              "      <td>0.934673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.065200</td>\n",
              "      <td>0.180895</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.969388</td>\n",
              "      <td>0.871560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.065300</td>\n",
              "      <td>0.114961</td>\n",
              "      <td>0.972857</td>\n",
              "      <td>0.855856</td>\n",
              "      <td>0.969388</td>\n",
              "      <td>0.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.059200</td>\n",
              "      <td>0.075724</td>\n",
              "      <td>0.982857</td>\n",
              "      <td>0.921569</td>\n",
              "      <td>0.959184</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 700\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-1500 (score: 0.07572387903928757).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1576, training_loss=0.1699102254688437, metrics={'train_runtime': 2253.867, 'train_samples_per_second': 5.59, 'train_steps_per_second': 0.699, 'total_flos': 4781314879387200.0, 'train_loss': 0.1699102254688437, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model"
      ],
      "metadata": {
        "id": "p4m-NZyaf3he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir='./model'\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "_w1uGhgvfFuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7643a6d7-5095-4925-983a-235d1a7c6597"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model/tokenizer_config.json',\n",
              " './model/special_tokens_map.json',\n",
              " './model/spiece.model',\n",
              " './model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download model"
      ],
      "metadata": {
        "id": "3AqYLENjf7of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/model.zip /content/model\n",
        "files.download('model.zip')"
      ],
      "metadata": {
        "id": "8y-AqyaAf6sg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6d4619c9-4f21-4dbd-d21b-da57f4d744d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/model/ (stored 0%)\n",
            "  adding: content/model/tokenizer_config.json (deflated 50%)\n",
            "  adding: content/model/special_tokens_map.json (deflated 52%)\n",
            "  adding: content/model/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/model/spiece.model (deflated 49%)\n",
            "  adding: content/model/config.json (deflated 52%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a827a14c-9301-49ca-8fd2-89aeede126f7\", \"model.zip\", 435738377)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict "
      ],
      "metadata": {
        "id": "2uxxMHVA1LbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "import torch.nn.functional as F\n",
        "#this will predict one sentence each time\n",
        "def predict(content):\n",
        "\n",
        "    inputs = tokenizer(content,\n",
        "                       \n",
        "                       padding='max_length',\n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "    # move to gpu\n",
        "    ids = inputs[\"input_ids\"].to(device)\n",
        "    idt = inputs[\"token_type_ids\"].to(device)\n",
        "    mask = inputs[\"attention_mask\"].to(device)\n",
        "    # forward pass\n",
        "    outputs = model(ids,token_type_ids=idt,attention_mask=mask)\n",
        "    logits = outputs[0]\n",
        "    x = F.softmax(logits, dim=-1)\n",
        "    active_logits = logits.view(-1, model.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
        "    flattened_predictions = torch.argmax(active_logits,\n",
        "                                         axis=1)\n",
        "    return x.cpu().detach().numpy()[0][1], flattened_predictions.cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "uu8C9w_j1K08"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare test data"
      ],
      "metadata": {
        "id": "EZOrJGMV2YlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df.irsen_text.values.tolist()[7000:]\n",
        "real = df.claim_s.values.tolist()[7000:]\n",
        "len(sentences)"
      ],
      "metadata": {
        "id": "jFtUHXpF2W0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63850dd-adf3-4acd-d10c-fda422d416d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "589"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('i172_589_labels.txt','r') as f:\n",
        "  real = f.readlines()\n",
        "real = [int(t.strip()) for t in real]"
      ],
      "metadata": {
        "id": "5416sjB0xLQ4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre = []# predict label\n",
        "pre_pro = []# predict probility\n",
        "for i in sentences:\n",
        "  x = predict(i)\n",
        "  pre.append(x[1])\n",
        "  pre_pro.append(x[0])"
      ],
      "metadata": {
        "id": "W8IksOq92qug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d59543-22b1-471e-fdc6-712ea0a84b84"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate false negative and false postive"
      ],
      "metadata": {
        "id": "3Og0Fua13yWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xl_tf = []\n",
        "for i in range(len(real)):\n",
        "  if pre[i] == 0 and real[i] == 0:\n",
        "    xl_tf.append('TN')\n",
        "  elif pre[i] == 1 and real[i] == 1:\n",
        "    xl_tf.append('TP')\n",
        "  elif pre[i] == 1 and real[i] == 0:\n",
        "    xl_tf.append('FP')\n",
        "  elif pre[i] == 0 and real[i] == 1:\n",
        "    xl_tf.append('FN')"
      ],
      "metadata": {
        "id": "-0UfRQXt3wKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('xl_tf.txt','w') as f:\n",
        "  f.writelines([str(p)+'\\n' for p in xl_tf])"
      ],
      "metadata": {
        "id": "d_tC0xev4BO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show result"
      ],
      "metadata": {
        "id": "grVRfFm_3CMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('f1:'+str(f1_score(real, pre, average=None))+'\\n'+'recall:'+str(recall_score(real, pre, average=None))+'\\n'+'precision:'+str(precision_score(real, pre, average=None))+'\\n'+'accuracy:'+str(accuracy_score(real, pre))+'\\n')"
      ],
      "metadata": {
        "id": "Vh8YVvaL292U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962ebc5a-eeb8-4940-a08a-77807b6ecbb9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1:[0.98532495 0.9375    ]\n",
            "recall:[0.98121086 0.95454545]\n",
            "precision:[0.98947368 0.92105263]\n",
            "accuracy:0.9762308998302207\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(real, pre)"
      ],
      "metadata": {
        "id": "x3KXqewY3JI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd24bdb-3039-4ba2-fa59-8ef9d8d8e4b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[470,   9],\n",
              "       [  5, 105]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('i172pre_xl.txt','w') as f:\n",
        "  f.writelines([str(p)+'\\n' for p in pre])\n",
        "with open('i172pro_xl.txt','w') as f:\n",
        "  f.writelines([str(p)+'\\n' for p in pre_pro])"
      ],
      "metadata": {
        "id": "MGLoXfBDxZW1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show probability graph"
      ],
      "metadata": {
        "id": "R8H0L4yf3LiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fig, ax = plt.subplots(1,1,figsize = (10,5))\n",
        "ax.hist(np.array(pre_pro)[np.array(real) == 1], color = \"darkred\",bins = \"scott\", alpha = .5, edgecolor = \"red\")\n",
        "ax.hist(np.array(pre_pro)[np.array(real) == 0], color = \"darkgreen\",bins = \"scott\", alpha = .5, edgecolor = \"green\")"
      ],
      "metadata": {
        "id": "8E3Km4wC28Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of samples model gives probabilty more than .8 but real label are non-evidence\n",
        "print('num of samples have score more than 0.8 but are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.8)))\n",
        "print('num of samples have score more than 0.8 are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.8)))\n",
        "# how much confidence if the socore is higher than 0.8, we are 85.7% confident that the sentence is evidence if the score is higher than .8\n",
        "confi80 = str(round((np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.8))/(np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.8) + np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.8)),4))\n",
        "print('we are '+ confi80 +' confident that the sentence is evidence if the score is higher than .8')\n",
        "# number of samples model gives probabilty more than .6 less than .8 but real label is non-evidence\n",
        "num6080_nevid = np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.6) - np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.80)\n",
        "num6080_evid = np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.6) - np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.80)\n",
        "print('num of samples have score more than 0.6 and less than 0.8 but are non-evidence :'+ str(num6080_nevid))\n",
        "print('num of samples have score more than 0.6 and less than 0.8 are evidence :'+ str(num6080_evid))\n",
        "confi6080 = num6080_evid/(num6080_evid + num6080_nevid)\n",
        "print('we are '+ str(round(confi6080,4)) +' confident that the sentence is evidence if the score is higher than .6 and less than .8')"
      ],
      "metadata": {
        "id": "uCxH3vO3tfk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('num of samples have score more than 0.8 but are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2)))\n",
        "print('num of samples have score more than 0.8 are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.2)))\n",
        "confi20 = str(round((np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2))/(np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2) + np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.2)),4))\n",
        "print('we are '+ confi80 +' confident that the sentence is non-evidence if the score is higher than .2')\n",
        "num2040_nevid = np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.4) - np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2)\n",
        "num2040_evid = np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.4) - np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.2)\n",
        "print('num of samples have score more than 0.2 and less than 0.4 but are non-evidence :'+ str(num2040_nevid))\n",
        "print('num of samples have score more than 0.2 and less than 0.4 are evidence :'+ str(num2040_evid))\n",
        "confi2040 = num2040_nevid/(num2040_evid + num2040_nevid)\n",
        "print('we are '+ str(round(confi2040,4)) +' confident that the sentence is non-evidence if the score is higher than .2 and less than .4')"
      ],
      "metadata": {
        "id": "G_76HdfutomY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the wrong predictions. The end of sentence shows the real labels\n",
        "for i in range(len(real)):\n",
        "  if real[i] != pre[i]:\n",
        "    print(sentences[i] + ' claim' if real[i] else sentences[i] + ' noclaim')"
      ],
      "metadata": {
        "id": "UH1dwnICsOxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2923e74-c234-49cc-d263-04db5053c0a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "But the biggest effect had tooked place was draining the swamp. noclaim\n",
            "But the pythons was a big problem. noclaim\n",
            "the Burmese pythons were a threat to the everglades because they were eating all the small mammals. noclaim\n",
            "The other threat to the Everglades is The Burmese python. noclaim\n",
            "In my opinion an entire swamp is harder to recover from compared to a couple hundred snakes that eventually you will probably be able to hunt. claim\n",
            "So thats my opinion of which one is a bigger threat. noclaim\n",
            "In this 5 paragraph essay i will show you why the Burmese python is the biggest threat to the everglades. noclaim\n",
            "The Burmese is a big threat to the animals that live in the everglades. claim\n",
            "Another reason, the biggest threat to the Everglades is the Burmese python is they eat little animals. claim\n",
            "This is why the Burmese Python is a bigger issue in the everglades than having to drain the Everglades. claim\n",
            "That was the biggest threat. noclaim\n",
            "The Burmese pythons are a big threat because they can eat most of the animals that live in the Everglades. noclaim\n",
            "One reason the overpopulation of the Burmese Python is the worse and more important threat is that they can eat a lot. claim\n",
            "so the Burmese Python is also a very bog threat to the everglades, because there will be no animals to live in the everglades if the Burmese Python is snooping around in the everglades. noclaim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLbQ_y8Kxck7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}